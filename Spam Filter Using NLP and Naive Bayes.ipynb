{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Building a Spam Filter Using NLP and Naive Bayes\n",
    "\n",
    "In this project, I'm going to build a spam filter for SMS messages using the multinomial Naive Bayes algorithm. My goal is to write a program that classifies new messages with an accuracy greater than 80% — so I expect that more than 80% of the new messages will be classified correctly as spam or ham (non-spam).\n",
    "\n",
    "To train the algorithm, I'll use a dataset of 5,572 SMS messages that are already classified by humans. The dataset was put together by Tiago A. Almeida and José María Gómez Hidalgo, and it can be downloaded from the The UCI Machine Learning Repository.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "stopwords=nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the dataset\n",
    "df=pd.read_csv('SMSSpamCollection',sep='\\t',header=None,names=['Label','SMS'])\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We see that about 87% of the messages are ham, and the remaining 13% are spam\n",
    "df['Label'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomize the dataset\n",
    "df=df.sample(frac=1,random_state=100)\n",
    "#Training/Test split (80% for traning and 20% for test)\n",
    "training_set=df[:4458].reset_index(drop=True)\n",
    "test_set=df[4459:].reset_index(drop=True)\n",
    "\n",
    "print(training_set.shape)\n",
    "print(test_set.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and test sets are well randomized\n",
    "training_set['Label'].value_counts(normalize=True)\n",
    "test_set['Label'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before cleaning\n",
    "training_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After cleaning (removing punctutation and setting lower cases)\n",
    "training_set['SMS']=training_set['SMS'].str.replace(r'\\W+',' ').str.lower()\n",
    "training_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set['SMS']=training_set['SMS'].str.split()\n",
    "\n",
    "# Remove word in stopwords\n",
    "for row in training_set['SMS']:\n",
    "    for word in stopwords:\n",
    "        while word in row:\n",
    "            row.remove(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary=[]\n",
    "for row in training_set['SMS']:\n",
    "    for word in row:\n",
    "        vocabulary.append(word)\n",
    "vocabulary=set(vocabulary)\n",
    "vocabulary=list(vocabulary)\n",
    "len(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a vocabulary list containing unique words\n",
    "vocabulary=[]\n",
    "for row in training_set['SMS']:\n",
    "    for word in row:\n",
    "        if word not in stopwords:\n",
    "            vocabulary.append(word)\n",
    "vocabulary=set(vocabulary)\n",
    "vocabulary=list(vocabulary)\n",
    "len(vocabulary)\n",
    "# There are 7,783 unique words in all the messages of the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a new dataset that counts word in each message\n",
    "word_counts_per_sms = {unique_word: [0] * len(training_set['SMS']) for unique_word in vocabulary}\n",
    "\n",
    "for index, sms in enumerate(training_set['SMS']):\n",
    "    for word in sms:\n",
    "        if word in vocabulary:\n",
    "            word_counts_per_sms[word][index] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts_df=pd.DataFrame(word_counts_per_sms)\n",
    "word_counts_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the two data sets (training_set and word_counts_df)\n",
    "training_set_clean=pd.concat([training_set,word_counts_df],axis=1)\n",
    "training_set_clean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probability of spam and ham\n",
    "p_spam=training_set_clean['Label'].value_counts(normalize=True)[1]\n",
    "p_ham=training_set_clean['Label'].value_counts(normalize=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of words in spam messages\n",
    "n_spam=0\n",
    "for row in training_set_clean['SMS'][training_set_clean['Label']=='spam']:\n",
    "        n_spam += len(row)\n",
    "\n",
    "# Number of words in ham messages      \n",
    "n_ham=0\n",
    "for row in training_set_clean['SMS'][training_set_clean['Label']=='ham']:\n",
    "        n_ham += len(row)\n",
    "\n",
    "# Number of unique words        \n",
    "n_vocabulary=len(vocabulary)\n",
    "alpha=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Isolating spam and ham messages\n",
    "spam_messages=training_set_clean[training_set_clean['Label']=='spam']\n",
    "ham_messages=training_set_clean[training_set_clean['Label']=='ham']\n",
    "\n",
    "# Initiate paramters\n",
    "parameter_spam={}\n",
    "parameter_ham={}\n",
    "\n",
    "# Caculate parameters\n",
    "for word in vocabulary:\n",
    "    # Calculate probability of a word given spam messages\n",
    "    n_word_given_spam=spam_messages[word].sum()\n",
    "    p_word_given_spam=(n_word_given_spam+alpha)/(n_spam+alpha*n_vocabulary)\n",
    "    parameter_spam[word]=p_word_given_spam\n",
    "    \n",
    "    # Calculate probability of a word given ham messages\n",
    "    n_word_given_ham=ham_messages[word].sum()\n",
    "    p_word_given_ham=(n_word_given_ham+alpha)/(n_ham+alpha*n_vocabulary)\n",
    "    parameter_ham[word]=p_word_given_ham\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating A Function to Classify A New Message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a classifying function\n",
    "def classify_test_set(message):\n",
    "    message = re.sub(r'\\W+', ' ', message)\n",
    "    message = message.lower()\n",
    "    message = message.split()\n",
    "    for word in stopwords:\n",
    "        while word in message:\n",
    "            message.remove(word)\n",
    "    \n",
    "    p_spam_given_message=p_spam\n",
    "    p_ham_given_message=p_ham \n",
    "    for word in message:\n",
    "        if word in vocabulary:\n",
    "            p_word_given_spam=parameter_spam[word]\n",
    "            p_spam_given_message *= p_word_given_spam\n",
    "            \n",
    "            p_word_given_ham=parameter_ham[word]\n",
    "            p_ham_given_message *= p_word_given_ham\n",
    "      \n",
    "    if p_ham_given_message > p_spam_given_message:\n",
    "        return 'ham'\n",
    "    elif p_ham_given_message < p_spam_given_message:\n",
    "        return 'spam'\n",
    "    else:\n",
    "        return 'needs human classification'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measuring the Spam Filter's Accuracy Using the Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set['predicted']= test_set['SMS'].apply(classify_test_set)\n",
    "\n",
    "# The accuracy is close to 98.74%, which is really good.\n",
    "accuracy_score(test_set['Label'],test_set['predicted'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An Alternative Method (Using CountVectorizer and MultinomialNB from Sklearn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('SMSSpamCollection',sep='\\t',header=None,names=['Label','SMS'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=df['SMS']\n",
    "y=df['Label']\n",
    "\n",
    "# Split the data into train/test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.2,random_state=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline=Pipeline([('cv',CountVectorizer(stop_words=stopwords)),\n",
    "                   ('nb',MultinomialNB()),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.fit(X_train,y_train)\n",
    "predictions=pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test,predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Using CountVectorizer and MultinominalNB yields a similar result but takes a lot fewer steps. In addition, it allows us to apply other models such as random forests, k-neighbors, etc."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
